---
title: 'Information-Content-Informed Kendall-tau Correlation: Utilizing Missing Values'
author: 'Robert M Flight & Hunter NB Moseley'
date: '`r Sys.time()`'
output: 
  redoc::redoc
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## Abstract

Almost all correlation measures currently available are unable to handle missing values.
Missing values either get ignored completely by removing them, or may be imputed and used in the calculation of the correlation coefficient.
In both cases, the correlation value will be impacted from it's true value, whatever it should be based on the ideal of no missing data.
Missing values occur in real life data for a variety of reasons. In -omics data sets, the primary reasons are not being present in a sample, or present but below the detection limit.
Therefore, these missing data are not missing at random, but potentially encode some information by their "missingness".
We propose the information-content-informed Kendall-tau (ICI-Kt) correlation coefficient, that allows missing values to carry explicit information in determination of concordant and discordant pairs.
We show that the ICI-Kt allows the inclusion of missing data values using simulated and real data sets from RNA-seq experiments.
Finally, we show that approximate ICI-Kt values can be calculated using smaller subsets of large data sets with significant time savings.

The ICI-Kt correlation calculation is available as part of the visualizationQualityControl package at  https://github.com/MoseleyBioinformaticsLab/visualizationQualityControl.

## Introduction

Correlation as a measure of the relatedness or similarity of two or more sets of data has a long history.
It has been used and abused in various scientific fields since it's introduction, more recently for applications in the varied -omics' (genomics, transcriptomics, proteomics, and metabolomics).
Correlation of features (RNA transcripts, proteins, metabolites) may be used to determine whether the features are related, as well as to generate feature - feature interaction networks for various analyses [STRING, WCGNA, and others I'm sure].
Feature - feature correlation may also be used to inform which features are used for imputation of missing values [I know I've seen this somewhere ...].
Correlation of samples in -omics may be used for evaluating whether all the samples from a particular condition are truly related to each other, as well as to inform the choice of analysis, and to check for outliers prior to other statistical analysis [Flight & Wentzell, Gierlinski et al].

However, all analytical methods, and in particular the analytical methods used in -omics where so many analytes are being measured simultaneously; suffer from missing measurements.
Of course some analytes will be missing at random just because of some issue with the instrument or that particular sample, but a large amount of missing measurements are left-censored due to analytes being below the detection limit of the instrument.
Imputation of missing measurements in -omics samples is an active area of research, which we will not cover here beyond to say that it is worthwhile and very necessary in many instances.
When it comes to calculating correlation, there are very few methods that explicitly account for missing data that we know of.
In many cases, missing values may be imputed to zero (or another value) and then included in the correlation calculation.
Imputation or replacement with zero is likely to lead to high inflation of Pearson and related correlation measures.
Other methods might be to keep only those features with no missing values across samples (complete cases), or keep non-missing between any pair of samples (pairwise complete cases).
Each of these are likely to mis-estimate the real sample - sample correlation values.

Assuming that a majority of missing values are **not missing at random**, but rather result from left-censored distributions due to the analyte being below the detection limit, we propose that these missing values do in fact encode useful information that can be incorporated into correlation calculations, especially the Kendall-tau paired rank correlation coefficient.
In this manuscript we propose new definitions of concordant and discordant rank pairs that include missing values, as well as methods for incorporating missing values into the number of tied values for the equivalent of the modified Kendall-tau coefficient.
We examine the effect of missing values on simulated data, comparing our information-content-informed Kendall-tau (ICI-Kt) method with other simple methods of handling the missing values, namely removing them or imputing them to zero.
Finally, we examine the ability to recapitulate ICI-Kt correlation values calculated on a large feature -omics data set using feature subsets, which is useful as the run time of large feature data sets can become prohibitive.

## Methods

### Implementation Details

A reference implementation where the various concordant and discordant pair definitions are written as simple logical tests in R was produced to allow further exploration of methods for speeding up the calculation.
In practice, it is possible to replace the missing values with a value that is smaller than all of the values in the two sets of values under consideration, and then do tests of signs to define concordant and discordant pairs (see the definitions of concordant and discordant pairs in Results).
The special cases for ties can then be tested and counted and included or rejected depending on whether the "local" or "global" correlation definitions are desired.

C++ and R code implementations are in the src/kendallc.cpp and R/kendalltau.R files of the visualizationQualityControl R package, hosted at https://github.com/MoseleyBioinformaticsLab/visualizationQualityControl.
The version of the code used in this manuscript is available on figshare.

### Simulated Samples

Simulated samples are generated by drawing 1000 random values from a log-normal distribution with a mean of 1 and sd of 0.5 and sorting them.
The negative sample has values multiplied by -1.
Missing value indices are generated by randomly sampling up to 499 of the lowest values in each sample.
The missing indices are replaced with NA, and then calculated the correlation between the samples.

Another group of two samples are generated by drawing 1000 random values from a log-normal distribution, and adding noise from a normal distribution with a mean of zero and standard deviation of 0.2 to create two samples.
Missing values are generated to these samples two ways: 1) by setting various intensity cutoffs from 0 to 1.5 in 0.1 increments and setting any values below the cutoff to missing; 2) randomly sampling indices in the two-sample matrix in increments of zero to 300 in increments of 50 and setting the indices to missing.

### EGFR RNA-Seq Data

RNA-Seq dataset from null and knock-in EGFR mice mutants. **going to need more than this**.

### Adenocarcinoma Recount RNA-Seq Data

We downloaded the V2 Recount lung cancer data, extracted the scaled counts, and trimmed to the Stage I adenocarcinoma samples, and those genes that have a non-zero count in at least one of the samples.

Pearson and Kendall correlations are calculated with zero, and replacing zero values with NA.

### Multiprocessing

When a large number of samples need to be compared, it may be useful to split the comparisons across compute instances (either hyperthreaded or physical cores or their combination, or physical compute clusters).
The *furrr* R package makes the definition of compute clusters easy.
For a large matrix computation, we first define the sample - sample comparisons to be performed, and then check how many instances of compute are available (defined by a previous call to *furrr*), split the comparisons into a list that can be easily distributed using *furrr::future_map*.

### Subsampling of Features

In addition to using the full set of features to calculate sample - sample correlations, it is possible to select subsamples of the features and only use them to calculate correlations.
Three methods were implemented: 1 - random selections; 2 - top set of features by variance; 3 - top set of features by variance of principal components.

For (1), random selection, feature subsamples of 1 - 5%, and then in 5% increments to 90% were taken using the *sample* R function.
For (2), selection by variance, feature variances across samples were calculated, and then the features ranked by their variances.
For (3), selection by variance of the principal components.
Using the full data set, PCA decomposition was performed, and the variance contribution of each PC calculated.
The percent variance was used to define the number of features with high loadings on that PC to take to make up the total subsample, with each PC contributing relatively less and less features.
Both the feature variance and PC variance subsample methods were run with subsamples of 30% and 50% of the features.

In all subsamples, the sample - sample correlation differences to those generated with the full feature set were compared, and the median and standard deviation of all of the correlation differences calculated to evaluate the performance of the subsampling.


### Computing Environment

All calculations were run on desktop machines with Intel i7-4930K CPUs clocked at 3.4GHz, with 6 physical cores and 12 available using hyperthreading, and 64GB of RAM.
The *drake* package was used to manage calculation dependencies, with each correlation of samples using internal multiprocessing via the *furrr* and *future* packages.

## Results

### Definition of Concordant and Discordant Pairs

In the simplest form, the Kendall-tau correlation can be defined as:

$$\tau = \frac{ | pairs_{concordant}  | - | pairs_{discordant}  |}{\left | pairs_{concordant} \right | + \left | pairs_{discordant} \right |}$$
In this case a **pair** are any two $x$ and $y$ points from two different samples, consisting of $(x_i, x_j)$ and $(y_i, y_j)$.
A concordant pair has the following classical definition:

  * $x_i > x_j$ and $y_i > y_j$ 
  * $x_i < x_j$ and $y_i < y_j$

A discordant pair has the following classical definition:

  * $x_i > x_j$ and $y_i < y_j$
  * $x_i < x_j$ and $y_i > y_j$

Assuming that the majority of missing values are not missing at random, but due to being below the detection limit, we can expand the concordant and discordant pair definitions to include missing values. Here $!x$ indicates a missing value.
Information-content-informed concordant pair definitions:

  * $x_i > x_j$ and $y_i > y_j$
  * $x_i < x_j$ and $y_i < y_j$
  * $x_i > x_j$ and $y_i \& !y_j$
  * $x_i < x_j$ and $!y_i \& y_j$
  * $x_i \& !x_j$ and $y_i > y_j$
  * $!x_i \& x_j$ and $y_i < y_j$ 
  * $x_i \& !x_j$ and $y_i \& !y_j$ 
  * $x_i \& x_j$ and $!y_i \& y_j$ 
  
Information content informed discordant pair definitions:

  * $x_i > x_j$ and $y_i < y_j$ 
  * $x_i < x_j$ and $y_i > y_j$
  * $x_i > x_j$ and $!y_i \& y_j$ 
  * $x_i < x_j$ and $y_i \& !y_j$
  * $x_i \& !x_j$ and $y_i < y_j$
  * $!x_i \& x_j$ and $y_i > y_j$
  * $x_i \& !x_j$ and $!y_i \& y_j$
  * $!x_i \& x_j$ and $y_i \& !y_j$

### Considering Ties

Tied values contribute to neither of the concordant or discordant pair counts. 
The original Kendall-tau formula can be easily modified to handle ties by adding the tied $x$ and $y$ values to the denominator, and in our special case of missing data, we can add the ties that result from both of $(x_i, x_j)$ and $(y_i, y_j)$ being missing.

$$\tau = \frac{\left | pairs_{concordant} \right | - \left | pairs_{discordant} \right |}{\left | pairs_{concordant} \right | + \left | pairs_{discordant} \right | + \left | pairs_{xties} \right | + \left | pairs_{yties} \right | + \left | pairs_{missing} \right |}$$

In the first instance, we remove those cases where both of the pairs are missing, as well as the following concordant and discordant pairs:

  * $x_i \& !x_j$ and $y_i \& !y_j$ 
  * $x_i \& x_j$ and $!y_i \& y_j$ 
  * $x_i \& !x_j$ and $!y_i \& y_j$
  * $!x_i \& x_j$ and $y_i \& !y_j$

We refer to this case as the "local" ICI-Kt correlation.
It is most appropriate for the comparison of only two samples, where we are concerned with what values are present in the two samples, with the odd case of missingness.

The other case, where we leave the above definitions in, as well as the ties resulting from both $x$ and $y$ being missing, we refer to as the "global" ICI-Kt correlation.
In this case, every single sample-sample correlation will consider the same number pair comparisons.
This is more useful when analyzing correlations across multiple samples, not just two sets of data.

### Theoretical Maxima

The "global" case also provides an interesting property, whereby we can calculate the theoretical maximum correlation that would be possible to observe given the lowest number of shared missing values.
This value can be useful to scale the rest of the observed correlation values across many sample - sample correlations, providing a value that scales an entire dataset appropriately.

$$cor_{max} = \frac{\binom{n-m}{2} + n * m}{\binom{n-m}{2} + n * m + \binom{m}{2}}$$
To calculate the theoretical maximum correlation for rescaling, we choose the two samples with the least number of missing values across the dataset.
Then $n$ is the length of the variables, $m$ is the count of missing values across the two samples divided by two and rounded towards zero.


### Comparison To Other Correlation Measures

We compared the ICI-Kt correlation to both Pearson and regular Kendall correlations with simulated missing data, as well as a large RNA-seq dataset.
The simplest simulated samples are to create three samples, where two are perfectly correlated, and the second perfectly anti-correlated, and replace the values in each systematically with missing values (in this case NA), and compute the ICI-Kt, Pearson, and Kendall correlations.

```{r compare_simple}
loadd(positive_kt)
loadd(positive_pearson)
positive_pearson[is.na(positive_pearson)] = 0

compare_df = data.frame(ici_kt = positive_kt,
                        pearson = positive_pearson)
ggplot(compare_df, aes(x = pearson, y = ici_kt)) + geom_point()
```

**I'm not sure if I'll include these, because these were mostly done to check that the calculations were correct**.

A slightly more realistic comparison is to create two samples with 1000 observations each drawn from a log-normal distribution, and sorted in each case.
The *true* correlation for each of the Kendall and Pearson correlations can be calculated, and then missingness introduced in the lower range of values (see Methods).

```{r compare_realistic}
loadd(realistic_sample_1)
loadd(realistic_sample_2)
loadd(realistic_neg_sample)
ref_pearson = cor(realistic_sample_1, realistic_sample_2)
ref_kendall = visqc_ici_kendallt(rbind(realistic_sample_1, realistic_sample_2), exclude_0 = FALSE, scale_max = FALSE)$cor[2,1]

ref_pearson_neg = cor(realistic_sample_1, realistic_neg_sample)
ref_kendall_neg = visqc_ici_kendallt(rbind(realistic_sample_1, realistic_neg_sample), exclude_0 = FALSE, scale_max = FALSE)$cor[2,1]

loadd(realistic_na)
n_na = purrr::map_int(realistic_na, length)
loadd(realistic_positive_pearson)
loadd(realistic_positive_kendall)
loadd(realistic_positive_kt)
loadd(realistic_negative_pearson)
loadd(realistic_negative_kendall)
loadd(realistic_negative_kt)
positive_df = rbind(data.frame(cor = realistic_positive_pearson,
                                diff = ref_pearson - realistic_positive_pearson,
                                na = n_na,
                                type = "pearson",
                                dir = "positive"),
                     data.frame(cor = realistic_positive_kendall,
                                diff = ref_kendall - realistic_positive_kendall,
                                na = n_na,
                                type = "kendall",
                                dir = "positive"),
                     data.frame(cor = realistic_positive_kt,
                                diff = ref_kendall - realistic_positive_kt,
                                na = n_na,
                                type = "icikt",
                                dir = "positive"))
negative_df = rbind(data.frame(cor = realistic_negative_pearson,
                                diff = ref_pearson_neg - realistic_negative_pearson,
                                na = n_na,
                                type = "pearson",
                                dir = "negative"),
                     data.frame(cor = realistic_negative_kendall,
                                diff = ref_kendall_neg - realistic_negative_kendall,
                                na = n_na,
                                type = "kendall",
                                dir = "negative"),
                     data.frame(cor = realistic_negative_kt,
                                diff = ref_kendall_neg - realistic_negative_kt,
                                na = n_na,
                                type = "icikt",
                                dir = "negative"))
pos_diff = ggplot(positive_df, aes(x = na, y = diff)) + 
  geom_point() +
  facet_grid(type ~ ., scales = "free") +
  labs(subtitle = "Positive Correlation")
neg_diff = ggplot(negative_df, aes(x = na, y = diff)) +
  geom_point() +
  facet_grid(type ~ ., scales = "free") +
  labs(subtitle = "Negative Correlation")
pos_diff + neg_diff
```

**Fig X**. The effect on correlation as increasing numbers of entries in either or both samples are changed to NA for ICI-Kt, Kendall, and Pearson.

In **Figure X**, we can see that as missing values are added, only the ICI-Kt correlation values change in any significant way.
As the number of missing values increase, the ICI-Kt values drop or increase by up to 0.2.
Similarly, Pearson correlation is also affected, but the degree of change in the correlation values are much less, on the order of only 1x10-4 for the positive case.

### Utility in Large Omics' Data Sets

#### Examples of Finding Outliers

**I can't figure out which of Christine's datasets this was for anymore ...**


#### Using Random Subsamples

```{r pretty_print}
pretty_print = function(in_val){
  sprintf(in_val, fmt = '%#.1f')
}
```

```{r load_data}
loadd(transcript_data)
loadd(ref_cor)
```

```{r load_ici_timings}
loadd(run_random_select_random_fraction_0.1)
loadd(run_random_select_random_fraction_0.3)
diff_vals = data.frame(ref = extract_data(ref_cor),
                     f_1 = extract_data(run_random_select_random_fraction_0.1),
                     f_3 = extract_data(run_random_select_random_fraction_0.3))
diff_vals = diff_vals %>%
  dplyr::mutate(diff_1 = f_1 - ref,
                diff_3 = f_3 - ref)
ref_f1_f3 = 
  (ggplot(diff_vals, aes(x = ref, y = f_1)) + geom_point() +
     labs(x = "full correlation", y = "10% features") +
     coord_equal()) /
  (ggplot(diff_vals, aes(x = ref, y = f_3)) + geom_point() +
     labs(x = "full correlation", y = "30% features") +
     coord_equal())
diff_f1_f3 = 
  (ggplot(diff_vals, aes(x = diff_1)) +
     geom_histogram(bins = 100) +
     labs(x = "10% features") +
     coord_cartesian(xlim = c(-0.01, 0.01))) /
  (ggplot(diff_vals, aes(x = diff_3)) +
     geom_histogram(bins = 100) +
     labs(x = "30% features") +
     coord_cartesian(xlim = c(-0.01, 0.01)))

f1_f3_plot = ref_f1_f3 | diff_f1_f3
f1_f3_plot
```

**Fig X**. Differences of correlation calculated for the ICI-Kt method using the full set of features and 10% and 30% of features as a scatterplot (left) and histogram (right).

```{r variables_as_percent}
loadd(combined_random)
min_sd = combined_random %>%
  dplyr::pull(sd) %>%
  min()
combined_random = combined_random %>%
  dplyr::mutate(sd_ratio = sd / min_sd)
long_random = combined_random %>%
  dplyr::mutate(`Log2(Run Time)` = log2(time),
                `Median(diff)` = median,
                `Ratio(SD(diff))` = sd_ratio) %>%
  dplyr::select(-sd, -time, -median, -sd_ratio) %>%
  tidyr::pivot_longer(cols = !c(type, fraction),
                      names_to = "measure",
                      values_to = "value") 
random_plot = ggplot(long_random, aes(x = fraction, y = value)) + 
  geom_line() +
  facet_wrap(~ measure, scales = "free", nrow = 1)

loadd(combined_rand_multiple)
combined_rand_multiple  = combined_rand_multiple %>%
  dplyr::mutate(`Median(diff)` = median,
                SD = sd,
                `Run Time` = time) %>%
  dplyr::select(-median, -sd, -time)
long_rand_multiple = tidyr::pivot_longer(combined_rand_multiple, cols = !c(fraction, type), names_to = "measure", values_to = "value")

replicate_plot = ggplot(long_rand_multiple, aes(x = value)) + geom_histogram() +
  facet_wrap(~ measure, scales = "free", nrow = 1)

random_plot / replicate_plot
```

**Fig X**. Top: Fraction of features used vs the log2 of the run time, the median value of the sample-sample correlation differences, and the standard deviation of the sample-sample correlation differences as a ratio of the standard deviation for the 90% features case. 
Sample-sample correlation differences are computed with respect to the correlations generated using all features. 
Bottom: Median differences, standard deviations and times to compute the sample-sample correlations for 20 different random feature samples using 10% of the features.

Given the length of time it takes to run the calculations for larger datasets (in particular RNA-Seq data), we wondered if "good enough" results could be obtained using subsamples of the features across the samples.
Our starting data is a set of stage I (I, Ia, and Ib) adenocarcinoma tumor and normal tissue RNA-Seq samples from the *recount2* project, with `r nrow(transcript_data)` features that have a non-zero value in at least one sample, and `r ncol(transcript_data)` samples.
Using the full set of features, the sample - sample correlations were calculated in `r pretty_print(ref_cor$run_time / 60)`  minutes, or `r pretty_print(ref_cor$run_time / 3600)` hours (on an 80 core machine).
This translates to days on machines with fewer cores.

Given such long run times, we decided to investigate the effect of subsampling features to decrease run time while still retaining reasonable estimates of the sample - sample correlations.
Figure X plots the feature subsample correlations for 10% and 30% of the features against those generated by the full set (left), as well as the histogram of the differences in values (right).
The plot of the changes in standard deviation (SD) of the differences as a function of the percentage of subsampled features in Fig X(top, Ratio(SD(diff))) demonstrates that the SD quickly goes towards the minimum ratio of the minimum ratio of SD, while the run time similarly increases extremely quickly (time is on a log2 scale, so each unit is a doubling of the run time).

Using the 10% level of feature subsampling, we also repeated the calculations 20 times with new subsets to verify that the results were not dependent on any particular subsample of features (Figure X (bottom)).

Similar graphs of the median and SD for versions of Pearson and Kendall correlation are included in the supplemental materials.

#### Dependence on Number of Samples

Finally, we wanted to characterize the time required as a function of the number of samples, and therefore the number of sample - sample comparisons.
For this, we ran with small numbers of samples **without** parallelization enabled, and then with larger numbers of samples **with** parallelization enabled.

```{r dependence_number}
loadd(combined_small)
loadd(combined_big)
combined_small = combined_small %>%
  dplyr::mutate(set = "small")
combined_big = combined_big %>%
  dplyr::mutate(set = "large")
all_samples = rbind(combined_small, combined_big)
all_samples$set = factor(all_samples$set, levels = c("small", "large"), ordered = TRUE)

ggplot(all_samples, aes(x = n_sample, y = log2(time))) + 
  geom_line() +
  facet_wrap(~ set, nrow = 1, scales = "free_x") +
  labs(x = "Number of Samples",
       y = "Log2(Time) (s)")
```

**Fig X**. Time taken by the ICI-Kt as a function of the number of samples, using 2 - 40 samples without parallelization, and 50 - 250 samples with parallelization.

As samples are added, the run time quickly increases, much as the number of features added also quickly increases.
With enough cores (80 in this case), the increase in time as samples are added increases in a similar way, and in fact the 6.5 times number of samples can be run in a similar amount of time as the 40 samples on a single core.

### Discussion

???

### Conclusions

???

## Author Contributions

RMF wrote the code in the visualizationQualityControl R package, tested the ICI-Kt correlation code, and wrote all of the analysis code for this manuscript. HNBM conceived of the ICI-Kt correlation, provided input into code structures, provided supervision of the analyses and interpretation of results. Both authors contributed to the writing of the manuscript.
